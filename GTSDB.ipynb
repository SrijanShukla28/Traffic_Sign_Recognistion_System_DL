{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a62695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\DL_Project\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)   # must match the .venv path from step 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7817a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc06ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('trains/images', exist_ok=True)\n",
    "os.makedirs('trains/imagesf', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e1d36ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'gt.txt' file copied successfully to: C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\annotations\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "#  Source file (your annotation file)\n",
    "source_file = r\"C:\\Users\\Asus\\Desktop\\DL Data_Set\\DLDB\\train\\TrainIJCNN2013\\gt.txt\"\n",
    "\n",
    "#  Destination folder in your VS Code project\n",
    "destination_dir = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\annotations\"\n",
    "\n",
    "#  Make sure destination folder exists\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "#  Copy the file\n",
    "shutil.copy(source_file, destination_dir)\n",
    "\n",
    "print(\" 'gt.txt' file copied successfully to:\", destination_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665ecc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1453 .ppm files.\n",
      " All .ppm files copied successfully to: C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "#  Specify your local source directory and file type\n",
    "source_dir = r\"C:\\Users\\Asus\\Desktop\\DL Data_Set\\DLDB\\train\\TrainIJCNN2013\"\n",
    "file_type = \"*.ppm\"\n",
    "\n",
    "#  Specify your destination directory inside your VS Code project\n",
    "destination_dir = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\"\n",
    "\n",
    "#  Create destination directory if it doesn’t exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "#  Find all .ppm files (recursively, includes subfolders)\n",
    "files = glob.glob(os.path.join(source_dir, \"**\", file_type), recursive=True)\n",
    "\n",
    "print(f\"Found {len(files)} .ppm files.\")\n",
    "\n",
    "#  Copy each file to the destination directory\n",
    "for file in files:\n",
    "    shutil.copy(file, destination_dir)\n",
    "\n",
    "print(\" All .ppm files copied successfully to:\", destination_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719dd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique image files in gt: 506\n",
      "Sample image: 00000.ppm\n",
      "Annotations for sample: [[774.0, 411.0, 815.0, 446.0, 11]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIG: update these if your paths are different ===\n",
    "# path to the gt.txt file (original annotations)\n",
    "gt_path = Path(r\"C:\\Users\\Asus\\Desktop\\DL Data_Set\\DLDB\\train\\TrainIJCNN2013\\gt.txt\")\n",
    "\n",
    "# optional: where you want to save a parsed CSV (not required)\n",
    "out_csv = Path(r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\annotations\\gt_parsed.csv\")\n",
    "\n",
    "# === helper to detect delimiter and parse lines ===\n",
    "def detect_and_parse_line(line: str):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        return None\n",
    "    # detect delimiter: prefer semicolon, then comma, otherwise whitespace\n",
    "    if \";\" in line:\n",
    "        parts = [p.strip() for p in line.split(\";\") if p.strip()]\n",
    "    elif \",\" in line:\n",
    "        parts = [p.strip() for p in line.split(\",\") if p.strip()]\n",
    "    else:\n",
    "        # split on any whitespace\n",
    "        parts = line.split()\n",
    "    return parts\n",
    "\n",
    "# === main parsing function ===\n",
    "def parse_gt_file(gt_path: Path):\n",
    "    if not gt_path.exists():\n",
    "        raise FileNotFoundError(f\"gt file not found: {gt_path}\")\n",
    "\n",
    "    annotations = {}  # filename -> list of [x1,y1,x2,y2,label]\n",
    "    with gt_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for ln_no, raw in enumerate(f, start=1):\n",
    "            parts = detect_and_parse_line(raw)\n",
    "            if not parts:\n",
    "                continue\n",
    "\n",
    "            # Expect first token = filename\n",
    "            filename = parts[0]\n",
    "            # Remaining tokens should contain coords and label.\n",
    "            # There are different possible layouts; try to be flexible:\n",
    "            # Common case: filename x1 y1 x2 y2 label (5 numbers after name)\n",
    "            try:\n",
    "                # try to parse the last 5 tokens as numbers (x1,y1,x2,y2,label)\n",
    "                last5 = parts[-5:]\n",
    "                x1, y1, x2, y2 = map(float, last5[:4])  # coordinates may be float\n",
    "                label = int(last5[4])\n",
    "            except Exception:\n",
    "                # If that didn't work, try parsing everything after filename as numbers and take sensible slicing\n",
    "                numbers = []\n",
    "                for token in parts[1:]:\n",
    "                    try:\n",
    "                        # Accept ints and floats\n",
    "                        if \".\" in token:\n",
    "                            numbers.append(float(token))\n",
    "                        else:\n",
    "                            numbers.append(int(token))\n",
    "                    except Exception:\n",
    "                        # non-numeric token (skip)\n",
    "                        pass\n",
    "                if len(numbers) >= 5:\n",
    "                    x1, y1, x2, y2 = map(float, numbers[:4])\n",
    "                    label = int(numbers[4])\n",
    "                else:\n",
    "                    # can't parse this line — skip with a warning\n",
    "                    print(f\"Warning: couldn't parse line {ln_no}: {raw.strip()}\")\n",
    "                    continue\n",
    "\n",
    "            target = [x1, y1, x2, y2, label]\n",
    "\n",
    "            # store in dictionary\n",
    "            if filename in annotations:\n",
    "                annotations[filename].append(target)\n",
    "            else:\n",
    "                annotations[filename] = [target]\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# === run it ===\n",
    "annotations = parse_gt_file(gt_path)\n",
    "print(\"Unique image files in gt:\", len(annotations))\n",
    "# example: show annotations for a sample image if present\n",
    "sample = next(iter(annotations)) if annotations else None\n",
    "if sample:\n",
    "    print(\"Sample image:\", sample)\n",
    "    print(\"Annotations for sample:\", annotations[sample])\n",
    "\n",
    "# === optional: write parsed CSV (one row per object) ===\n",
    "def dump_parsed_csv(annotations: dict, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"filename,x1,y1,x2,y2,label\\n\")\n",
    "        for fname, objs in annotations.items():\n",
    "            for (x1,y1,x2,y2,label) in objs:\n",
    "                f.write(f\"{fname},{x1},{y1},{x2},{y2},{label}\\n\")\n",
    "    print(\"Parsed CSV written to:\", out_path)\n",
    "\n",
    "# Uncomment to create parsed CSV\n",
    "# dump_parsed_csv(annotations, out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2242b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations...\n",
      "Found 852 total annotations.\n",
      "Loaded 506 images with mapped annotations.\n",
      "Number of Images: 506\n"
     ]
    }
   ],
   "source": [
    "# --- THIS IS THE CORRECTED CODE FOR YOUR 'dic' ---\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading annotations...\")\n",
    "# Corrected path (using raw string)\n",
    "txt = np.genfromtxt(r'C:\\Users\\Asus\\Desktop\\DL Data_Set\\DLDB\\train\\TrainIJCNN2013\\gt.txt',\n",
    "                  delimiter=';', dtype=None, encoding=None) # type: ignore\n",
    "\n",
    "# Create a dictionary with image names as key and annotations as value\n",
    "dic = {}\n",
    "print(f\"Found {len(txt)} total annotations.\")\n",
    "\n",
    "for i in range(len(txt)):\n",
    "    # Image name is the first element\n",
    "    img_name = txt[i][0]\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    # Your model needs labels from 1 to 43.\n",
    "    # Your data has labels from 0 to 42.\n",
    "    # We must add +1 to every label.\n",
    "    original_label = int(txt[i][5])\n",
    "    new_label = original_label + 1  # Map 0->1, 1->2, ..., 42->43\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    # Bounding box coordinates\n",
    "    bbox = [int(txt[i][1]), int(txt[i][2]), int(txt[i][3]), int(txt[i][4])]\n",
    "    \n",
    "    # Store as a DICTIONARY (as expected by dataset.py)\n",
    "    annotation = {'bbox': bbox, 'label': new_label} # <-- Use the NEW, +1 label\n",
    "    \n",
    "    # Store as list of dictionaries\n",
    "    if img_name in dic:\n",
    "        dic[img_name].append(annotation)\n",
    "    else:\n",
    "        dic[img_name] = [annotation]\n",
    "\n",
    "print(f\"Loaded {len(dic)} images with mapped annotations.\")\n",
    "print(\"Number of Images:\", len(dic))\n",
    "\n",
    "# --- END OF 'dic' CODE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03b319e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bbox': [983, 388, 1024, 432], 'label': 41},\n",
       " {'bbox': [386, 494, 442, 552], 'label': 39},\n",
       " {'bbox': [973, 335, 1031, 390], 'label': 14}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['00001.ppm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6cd24",
   "metadata": {},
   "source": [
    "## DATA DISTRIBUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dde5625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{12: 26, 41: 7, 39: 57, 14: 52, 40: 4, 5: 31, 10: 32, 22: 5, 3: 59, 13: 54, 2: 48, 26: 21, 31: 14, 24: 13, 28: 3, 36: 15, 16: 10, 34: 13, 29: 9, 19: 27, 37: 8, 27: 11, 38: 1, 35: 9, 1: 4, 25: 2, 15: 22, 21: 9, 30: 4, 7: 17, 11: 63, 9: 47, 6: 37, 17: 7, 20: 2, 18: 25, 4: 21, 8: 37, 42: 6, 32: 1, 23: 9, 43: 7, 33: 3}\n"
     ]
    }
   ],
   "source": [
    "cls_lst = {}\n",
    "\n",
    "for i in dic:\n",
    "    for j in dic[i]:\n",
    "        # --- THIS IS THE FIX ---\n",
    "        # Get the label by its key, not by its index\n",
    "        cls = j['label']  \n",
    "        # -----------------------\n",
    "\n",
    "        if cls in cls_lst:\n",
    "            cls_lst[cls] += 1\n",
    "        else:\n",
    "            cls_lst[cls] = 1\n",
    "\n",
    "print(cls_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95332e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "normalized = {int(k): v for k, v in cls_lst.items()}\n",
    "sorted_classes = sorted(normalized.keys())\n",
    "xx = [str(c) for c in sorted_classes]\n",
    "yy = [normalized[c] for c in sorted_classes]\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "x_pos = range(len(xx))\n",
    "plt.bar(x_pos, yy, color='green')\n",
    "plt.xticks(x_pos, xx, rotation=45, ha='right')\n",
    "plt.xlabel(\"Class Number\")\n",
    "plt.ylabel(\"Number of Examples\")\n",
    "plt.title(\"GTSDB — Number of Samples per Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0c4dd",
   "metadata": {},
   "source": [
    "### Organizing the GTSDB Dataset:\n",
    "Unzipping the dataset zip, there are class folders with respective images and .ppm files. There is also the annotation file named as gt.txt\n",
    "\n",
    "I had copied all the .ppm files to a directory named \"train/images\".\n",
    "\n",
    "In the code below, the images that had annotations were only copied to a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2e3da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Copied 506 annotated images to: C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\imagesf\n",
      " Number of images in destination folder: 506\n",
      " Number of annotated images in gt.txt: 506\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "#  Path where all .ppm images are currently located\n",
    "source_dir = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\"\n",
    "\n",
    "#  Path where only annotated images will be copied\n",
    "destination_dir = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\imagesf\"\n",
    "\n",
    "#  Make sure destination folder exists\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "#  'dic' is your dictionary from gt.txt with image names as keys\n",
    "# Example: dic = {'00001.ppm': [...], '00002.ppm': [...]}\n",
    "\n",
    "#  Copy only annotated files\n",
    "copied_count = 0\n",
    "for img_name in dic.keys():\n",
    "    source_file = os.path.join(source_dir, img_name)\n",
    "    destination_file = os.path.join(destination_dir, img_name)\n",
    "    if os.path.exists(source_file):\n",
    "        shutil.copyfile(source_file, destination_file)\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        print(f\"⚠️ File not found: {source_file}\")\n",
    "\n",
    "print(f\" Copied {copied_count} annotated images to: {destination_dir}\")\n",
    "\n",
    "#  Verify by counting .ppm files\n",
    "ppm_files = glob.glob(os.path.join(destination_dir, \"*.ppm\"))\n",
    "print(f\" Number of images in destination folder: {len(ppm_files)}\")\n",
    "print(f\" Number of annotated images in gt.txt: {len(dic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a32805",
   "metadata": {},
   "source": [
    "### Defining Custom Dataset Class\n",
    "The below code is to get all the required data from the dataset while reading. Here's what happens in the myDataset (torch.utils.data.Dataset) class:\n",
    "\n",
    "Initialize all the required variables: Root directory of images (path), transforms (boolean), imgs (images dir in root).\n",
    "According to PyTorch's documentation, the Dataset class should implement getitem and len methods. So we declare them.\n",
    "In the getitem method, for each image we take the annotations and labels as input from the dictionary we created before. We store them in 'objects' variable.\n",
    "A 'targets' dictionary is then initialized to pass all the data to the model while training.\n",
    "'area' declared for the evaluation metrics of COCO API. It separates the metric scores between small, medium and large boxes.</br> 'iscrowd=True' will ignore all instances with numerous objects in one image. </br> 'image_id' is an image identifier. It is unique between all images in the dataset and is used during evaluation.\n",
    "'transforms=True' will call the transforms function to apply transformations.\n",
    "len method returns the size of the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e509e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FIX 1: Add all necessary imports ---\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class myDataset(torch.utils.data.Dataset):\n",
    "    # --- FIX 2: Pass 'dic' into the constructor ---\n",
    "    def __init__(self, root, dic, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.dic = dic  # Store dic as a class attribute\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"imagesf\"))))\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"imagesf\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # --- FIX 3: Use 'self.dic' ---\n",
    "        objects = self.dic[self.imgs[idx]]\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in objects:\n",
    "            name = obj[-1]\n",
    "            # --- FIX 4: Use built-in int() and float() (np.int/np.float are deprecated) ---\n",
    "            labels.append(int(name))\n",
    "            \n",
    "            xmin = float(obj[0])\n",
    "            ymin = float(obj[1])\n",
    "            xmax = float(obj[2])\n",
    "            ymax = float(obj[3])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    " \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n",
    " \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    " \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    " \n",
    "        return img, target\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44095488",
   "metadata": {},
   "source": [
    "### Data Augmentation using PyTorch's Transforms\n",
    "The images are enhanced before being passed to the network. The images are transformed using the functions defined in the \"transforms.py\" file in pytorch/vision.\n",
    "\n",
    "The difference between original and transformed images are shown in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ca69d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for object detection.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "    \"\"\"\n",
    "    Learning rate warmup scheduler.\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    \"\"\"\n",
    "    Metric logger for training and evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = {}\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters.setdefault(k, AverageValueMeter()).update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.data:\n",
    "            return self.data[attr]\n",
    "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(f\"{name}: {str(meter)}\")\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = \"\"\n",
    "        start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        \n",
    "        if start_time:\n",
    "            start_time.record()\n",
    "        else:\n",
    "            import time\n",
    "            start_time_cpu = time.time()\n",
    "\n",
    "        for obj in iterable:\n",
    "            if start_time:\n",
    "                end_time.record()\n",
    "                torch.cuda.synchronize()\n",
    "                data_time = start_time.elapsed_time(end_time)\n",
    "            else:\n",
    "                end_time_cpu = time.time()\n",
    "                data_time = (end_time_cpu - start_time_cpu) * 1000  # in ms\n",
    "            \n",
    "            yield obj\n",
    "            \n",
    "            if start_time:\n",
    "                iter_time = start_time.elapsed_time(end_time)\n",
    "            else:\n",
    "                iter_time = (time.time() - end_time_cpu) * 1000 # in ms\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print(f\"{header}  Data: {data_time:.3f}ms  Iter: {iter_time:.3f}ms\")\n",
    "                # You might want to log other stats here\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            if start_time:\n",
    "                start_time.record()\n",
    "            else:\n",
    "                start_time_cpu = time.time()\n",
    "\n",
    "\n",
    "class AverageValueMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.avg:.4f} ({self.val:.4f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d48cf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "# Note: The new PyTorch v2 transforms are recommended\n",
    "# but this implementation maintains compatibility with the older\n",
    "# tutorial style where transforms are called with (image, target)\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Converts a PIL Image or numpy.ndarray to tensor.\"\"\"\n",
    "    def __call__(self, image, target):\n",
    "        image = T.functional.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    \"\"\"Horizontally flip the given image randomly with a given probability.\"\"\"\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.prob:\n",
    "            image = T.functional.hflip(image)\n",
    "            if \"boxes\" in target:\n",
    "                bbox = target[\"boxes\"]\n",
    "                # Get image width\n",
    "                _, _, width = T.functional.get_dimensions(image)\n",
    "                # Flip bounding boxes\n",
    "                bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "                target[\"boxes\"] = bbox\n",
    "            if \"masks\" in target:\n",
    "                target[\"masks\"] = T.functional.hflip(target[\"masks\"])\n",
    "            # Add keypoints flip if needed\n",
    "                \n",
    "        return image, target\n",
    "\n",
    "# This is the function your notebook calls\n",
    "def get_transforms(train=False):\n",
    "    \"\"\"\n",
    "    Returns the appropriate transforms for training or validation.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    # All datasets get converted to tensor\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        # Add horizontal flip for training data\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    # Note: Modern v2 transforms would also handle dtype conversion\n",
    "    # and normalization, but we stick to the tutorial's basics.\n",
    "    # You may need to add normalization if your model expects it.\n",
    "    \n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import torchvision.models.detection.mask_rcnn\n",
    "\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "import utils\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.AverageValueMeter())\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = {k: v for k, v in loss_dict.items()} # No distributed training here\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    return metric_logger\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    model.eval()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    header = \"Test:\"\n",
    "\n",
    "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        model_time = time.time()\n",
    "        outputs = model(images)\n",
    "\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        model_time = time.time() - model_time\n",
    "\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        \n",
    "        evaluator_time = time.time()\n",
    "        coco_evaluator.update(res)\n",
    "        evaluator_time = time.time() - evaluator_time\n",
    "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "\n",
    "    # accumulate predictions from all images\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator\n",
    "\n",
    "\n",
    "def _get_iou_types(model):\n",
    "    model_without_ddp = model\n",
    "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "        model_without_ddp = model.module\n",
    "    iou_types = [\"bbox\"]\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
    "        iou_types.append(\"segm\")\n",
    "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
    "        iou_types.append(\"keypoints\")\n",
    "    return iou_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73403b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "def get_coco_api_from_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Helper function to get COCO api instance from a dataset.\n",
    "    \"\"\"\n",
    "    # Check if dataset is a subset\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        dataset = dataset.dataset\n",
    "        \n",
    "    # Check if dataset is the COCO class\n",
    "    if isinstance(dataset, COCO):\n",
    "        return dataset\n",
    "\n",
    "    # Check if dataset has a 'coco' attribute\n",
    "    if hasattr(dataset, 'coco'):\n",
    "        return dataset.coco\n",
    "\n",
    "    # If dataset is a list or tuple of datasets (e.g. ConcatDataset)\n",
    "    if isinstance(dataset, (list, tuple)):\n",
    "        if all(hasattr(d, 'coco') for d in dataset):\n",
    "            # This is simplified: assumes all datasets share the same coco api\n",
    "            return dataset[0].coco\n",
    "\n",
    "    # Fallback: create a dummy COCO api\n",
    "    # This is necessary if the dataset doesn't have a COCO api (e.g., custom dataset)\n",
    "    # We create a simple one based on image_ids and categories\n",
    "    \n",
    "    # Try to get categories from the dataset\n",
    "    categories = []\n",
    "    if hasattr(dataset, 'get_categories'):\n",
    "         categories = dataset.get_categories()\n",
    "    elif hasattr(dataset, 'categories'):\n",
    "         categories = dataset.categories\n",
    "    else:\n",
    "         # Create dummy categories if not available (1-indexed)\n",
    "         # We need to know the number of classes.\n",
    "         # This part is tricky without a standard API.\n",
    "         # Let's assume the 'labels' in targets are category_ids\n",
    "         print(\"Warning: Creating dummy COCO API. Category mapping might be incorrect.\")\n",
    "         # Let's try to infer from the dataset's targets\n",
    "         all_labels = set()\n",
    "         if hasattr(dataset, 'get_image_info'): # common in custom datasets\n",
    "             for idx in range(len(dataset)):\n",
    "                 target = dataset.get_image_info(idx)\n",
    "                 if 'labels' in target:\n",
    "                     all_labels.update(target['labels'].tolist())\n",
    "         elif hasattr(dataset, '__getitem__'):\n",
    "             # This is slow, but a last resort\n",
    "             print(\"Inferring categories from dataset items... This might be slow.\")\n",
    "             for idx in range(min(len(dataset), 100)): # Check first 100\n",
    "                 try:\n",
    "                     _, target = dataset[idx]\n",
    "                     if 'labels' in target:\n",
    "                         all_labels.update(target['labels'].tolist())\n",
    "                 except:\n",
    "                     pass # __getitem__ might not return (img, target)\n",
    "         \n",
    "         if not all_labels:\n",
    "             # If we still have no labels, we can't build a good dummy\n",
    "             print(\"Could not infer categories. Evaluation might fail or be incorrect.\")\n",
    "             # Let's assume 91 classes as a default, which is COCO's default\n",
    "             all_labels = set(range(1, 92))\n",
    "             \n",
    "         categories = [{\"id\": int(i), \"name\": str(i), \"supercategory\": \"none\"} for i in sorted(list(all_labels))]\n",
    "\n",
    "\n",
    "    coco_gt = COCO()\n",
    "    coco_gt.dataset = {'images': [], 'annotations': [], 'categories': categories}\n",
    "    \n",
    "    image_ids = []\n",
    "    if hasattr(dataset, 'get_img_ids'):\n",
    "        image_ids = dataset.get_img_ids()\n",
    "    else:\n",
    "        image_ids = list(range(len(dataset))) # Use index as image_id\n",
    "\n",
    "    coco_gt.dataset['images'] = [{'id': img_id} for img_id in image_ids]\n",
    "\n",
    "    # Create dummy annotations (this is the minimal required structure)\n",
    "    ann_id = 1\n",
    "    for idx, img_id in enumerate(image_ids):\n",
    "        target = None\n",
    "        if hasattr(dataset, 'get_image_info'):\n",
    "            target = dataset.get_image_info(idx)\n",
    "        elif hasattr(dataset, '__getitem__'):\n",
    "            try:\n",
    "                _, target = dataset[idx]\n",
    "            except:\n",
    "                target = None # Not all datasets return target this way\n",
    "\n",
    "        if target is None:\n",
    "            continue\n",
    "            \n",
    "        boxes = target.get('boxes', [])\n",
    "        labels = target.get('labels', [])\n",
    "        \n",
    "        # Ensure boxes is a tensor, then tolist\n",
    "        if isinstance(boxes, torch.Tensor):\n",
    "            boxes = boxes.tolist()\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            labels = labels.tolist()\n",
    "\n",
    "        for box, label in zip(boxes, labels):\n",
    "            # convert [x1, y1, x2, y2] to [x1, y1, width, height]\n",
    "            box_xywh = [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n",
    "            ann = {\n",
    "                'id': ann_id,\n",
    "                'image_id': img_id,\n",
    "                'category_id': label,\n",
    "                'bbox': box_xywh,\n",
    "                'area': box_xywh[2] * box_xywh[3],\n",
    "                'iscrowd': 0 # default to 0\n",
    "            }\n",
    "            coco_gt.dataset['annotations'].append(ann)\n",
    "            ann_id += 1\n",
    "            \n",
    "    coco_gt.createIndex()\n",
    "    return coco_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9c56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "class CocoEvaluator:\n",
    "    def __init__(self, coco_gt, iou_types):\n",
    "        if not isinstance(iou_types, (list, tuple)):\n",
    "            iou_types = [iou_types]\n",
    "        coco_gt = copy.deepcopy(coco_gt)\n",
    "        self.coco_gt = coco_gt\n",
    "        self.iou_types = iou_types\n",
    "        self.coco_eval = {iou_type: COCOeval(coco_gt, iouType=iou_type) for iou_type in iou_types}\n",
    "        self.img_ids = []\n",
    "        self.eval_imgs = {iou_type: [] for iou_type in iou_types}\n",
    "\n",
    "    def update(self, predictions):\n",
    "        img_ids = list(predictions.keys())\n",
    "        self.img_ids.extend(img_ids)\n",
    "\n",
    "        for iou_type in self.iou_types:\n",
    "            results = self.prepare(predictions, iou_type)\n",
    "            \n",
    "            # Use loadRes directly\n",
    "            self.coco_eval[iou_type].cocoDt = self.coco_gt.loadRes(results)\n",
    "            self.eval_imgs[iou_type] = list(predictions.keys())\n",
    "\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        # No distributed processing, so this is a no-op\n",
    "        pass\n",
    "\n",
    "    def accumulate(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            print(f\"Accumulating evaluation results for iou_type: {iou_type}...\")\n",
    "            # Set the image IDs to evaluate\n",
    "            self.coco_eval[iou_type].params.imgIds = self.img_ids\n",
    "            # Run evaluation\n",
    "            self.coco_eval[iou_type].evaluate()\n",
    "            self.coco_eval[iou_type].accumulate()\n",
    "\n",
    "    def summarize(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            print(f\"IoU metric: {iou_type}\")\n",
    "            self.coco_eval[iou_type].summarize()\n",
    "\n",
    "    def prepare(self, predictions, iou_type):\n",
    "        if iou_type == \"bbox\":\n",
    "            return self.prepare_for_coco_detection(predictions)\n",
    "        elif iou_type == \"segm\":\n",
    "            return self.prepare_for_coco_segmentation(predictions)\n",
    "        elif iou_type == \"keypoints\":\n",
    "            return self.prepare_for_coco_keypoint(predictions)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown iou_type: {iou_type}\")\n",
    "\n",
    "    def prepare_for_coco_detection(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if \"boxes\" not in prediction or \"scores\" not in prediction or \"labels\" not in prediction:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"].tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                # convert [x1, y1, x2, y2] to [x1, y1, width, height]\n",
    "                box[2] = box[2] - box[0]\n",
    "                box[3] = box[3] - box[1]\n",
    "                \n",
    "                result = {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": label,\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": score,\n",
    "                }\n",
    "                coco_results.append(result)\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_segmentation(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if \"masks\" not in prediction or \"scores\" not in prediction or \"labels\" not in prediction:\n",
    "                continue\n",
    "\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "            masks = prediction[\"masks\"]\n",
    "            \n",
    "            # rle conversion\n",
    "            masks = masks.permute(0, 2, 3, 1).contiguous().permute(0, 3, 1, 2)\n",
    "            masks = mask_util.encode(masks.numpy())\n",
    "\n",
    "            for mask, score, label in zip(masks, scores, labels):\n",
    "                result = {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": label,\n",
    "                    \"segmentation\": mask,\n",
    "                    \"score\": score,\n",
    "                }\n",
    "                coco_results.append(result)\n",
    "        return coco_results\n",
    "\n",
    "    def prepare_for_coco_keypoint(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if \"keypoints\" not in prediction or \"scores\" not in prediction or \"labels\" not in prediction:\n",
    "                continue\n",
    "\n",
    "            keypoints = prediction[\"keypoints\"].tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            labels = prediction[\"labels\"].tolist()\n",
    "\n",
    "            for kps, score, label in zip(keypoints, scores, labels):\n",
    "                result = {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": label,\n",
    "                    \"keypoints\": kps,\n",
    "                    \"score\": score,\n",
    "                }\n",
    "                coco_results.append(result)\n",
    "        return coco_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, dic=None, exts=(\".jpg\",\".jpeg\",\".png\",\".ppm\",\".bmp\",\".tif\",\".tiff\")):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.exts = exts\n",
    "\n",
    "        # collect images recursively\n",
    "        self.files = []\n",
    "        for dp, dn, filenames in os.walk(self.root):\n",
    "            for f in filenames:\n",
    "                if f.lower().endswith(self.exts):\n",
    "                    self.files.append(os.path.join(dp, f))\n",
    "        self.files = sorted(self.files)\n",
    "        print(f\"myDataset: found {len(self.files)} images under {self.root}\")\n",
    "\n",
    "        # annotation dict keyed by basename (optional)\n",
    "        self.dic = dic if dic is not None else {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return (image, target) where:\n",
    "         - image is a PIL image or transformed tensor\n",
    "         - target is a dict with keys boxes, labels, image_id, area, iscrowd\n",
    "        This handles transforms that expect either (img) or (img, target).\n",
    "        \"\"\"\n",
    "        img_path = self.files[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # prepare target from annotation dict BEFORE transforms (so transforms that modify boxes can use it)\n",
    "        basename = os.path.basename(img_path)\n",
    "        objects = self.dic.get(basename, [])\n",
    "\n",
    "        if len(objects) == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.tensor([o['bbox'] for o in objects], dtype=torch.float32)\n",
    "            labels = torch.tensor([o['label'] for o in objects], dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if boxes.numel() else torch.tensor([])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": area,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        # Apply transforms: try (img, target) first; if transform expects only image, fall back.\n",
    "        if self.transform:\n",
    "            try:\n",
    "                # many detection transforms return (img, target)\n",
    "                img, target = self.transform(img, target)\n",
    "            except TypeError:\n",
    "                # fallback: transform accepts only image and returns image\n",
    "                img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "016c226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in c:\\users\\asus\\desktop\\dl_project\\.venv\\lib\\site-packages (2.0.10)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\desktop\\dl_project\\.venv\\lib\\site-packages (from pycocotools) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "%pip install pycocotools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7250370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import transforms as T\n",
    "from engine import train_one_epoch, evaluate\n",
    "# utils, transforms, engine were just downloadedUtils.py,transforms.py,engine.py\n",
    " \n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        # 50% chance of flipping horizontally\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    " \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d50ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python working dir: c:\\Users\\Asus\\Desktop\\DL_Project\n",
      "Python sys.path (first entries):\n",
      "  C:\\Python 3.10\\python310.zip\n",
      "  C:\\Python 3.10\\DLLs\n",
      "  C:\\Python 3.10\\lib\n",
      "  C:\\Python 3.10\n",
      "\n",
      "Files in project root:\n",
      "['.venv', 'coco_eval.py', 'coco_utils.py', 'data', 'dataset.py', 'engine.py', 'GTSDB.ipynb', 'hub', 'requirements.txt', 'trains', 'transforms.py', 'utils.py', '__pycache__']\n",
      "\n",
      "Files in src folder: <no src folder>\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "print(\"Python working dir:\", os.getcwd())\n",
    "print(\"Python sys.path (first entries):\")\n",
    "for p in sys.path[:4]:\n",
    "    print(\" \", p)\n",
    "\n",
    "# list files in your project folder (update path if your project folder is elsewhere)\n",
    "proj = r\"C:\\Users\\Asus\\Desktop\\DL_Project\"\n",
    "print(\"\\nFiles in project root:\")\n",
    "print(os.listdir(proj))\n",
    "\n",
    "# list files in possible 'src' folder\n",
    "src = os.path.join(proj, \"src\")\n",
    "print(\"\\nFiles in src folder:\", os.path.exists(src) and os.listdir(src) or \"<no src folder>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import myDataset\n",
    "from transforms import get_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2119d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model with COCO head\n",
      "Replaced box predictor -> new num_classes = 44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=44, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=176, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "num_classes = 44  # number of classes in your dataset (NOT including background? For torchvision it's the total number of classes including background)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 1) create model with pretrained weights (default weights include a COCO head sized for 91 classes)\n",
    "#    do NOT pass num_classes here (so torchvision doesn't validate it)\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "print(\"Loaded pretrained model with COCO head\")\n",
    "\n",
    "# 2) replace the box predictor with a new one for your num_classes\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "print(f\"Replaced box predictor -> new num_classes = {num_classes}\")\n",
    "\n",
    "# 3) move to device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ccfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19640441",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\"   # folder containing imagesf/\n",
    "GT_PATH = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\annotations\\gt.txt\"  # your annotation file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations for 506 images.\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "with open(GT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(';')\n",
    "        if len(parts) < 6:\n",
    "            continue\n",
    "        img_name = parts[0]\n",
    "        coords = [float(parts[1]), float(parts[2]), float(parts[3]), float(parts[4]), int(parts[5])]\n",
    "        dic.setdefault(img_name, []).append(coords)\n",
    "\n",
    "print(\"Loaded annotations for\", len(dic), \"images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNotebookDataset(Dataset):\n",
    "    def __init__(self, root, dic_annotations, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.dic = dic_annotations\n",
    "        imgs_dir = os.path.join(root, \"imagesf\")\n",
    "        self.imgs = list(sorted([f for f in os.listdir(imgs_dir) if f.lower().endswith('.ppm')]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.imgs[idx]\n",
    "        img_path = os.path.join(self.root, \"imagesf\", img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        objects = self.dic.get(img_name, [])\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        for obj in objects:\n",
    "            xmin, ymin, xmax, ymax = map(float, obj[:4])\n",
    "            cls = int(obj[-1])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(cls)\n",
    "\n",
    "        if boxes:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.zeros((0,4), dtype=torch.float32)\n",
    "            labels = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:,3] - boxes[:,1]) * (boxes[:,2] - boxes[:,0]) if boxes.numel() else torch.zeros((0,), dtype=torch.float32)\n",
    "        iscrowd = torch.zeros((len(labels),), dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": image_id, \"area\": area, \"iscrowd\": iscrowd}\n",
    "\n",
    "        import torchvision.transforms as T\n",
    "        img = T.ToTensor()(img)\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "DataLoader ready, total batches: 253\n",
      "Model loaded and moved to device.\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "dataset = MyNotebookDataset(ROOT, dic)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "print(\"DataLoader ready, total batches:\", len(data_loader))\n",
    "\n",
    "# Define the model\n",
    "num_classes = 44  # adjust this as per your project\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "print(\"Model loaded and moved to device.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"numpy\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da315eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\Users\\Asus\\Desktop\\DL_Project\\.venv\\Scripts\\python.exe\n",
      "Numpy import OK: 1.24.4 file: c:\\Users\\Asus\\Desktop\\DL_Project\\.venv\\lib\\site-packages\\numpy\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Numpy import OK:\", np.__version__, \"file:\", np.__file__)\n",
    "except Exception as e:\n",
    "    print(\"Numpy import FAILED:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4960628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\Desktop\\DL_Project\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: c:\\Users\\Asus\\Desktop\\DL_Project\\.venv\\Scripts\\python.exe\n",
      "python version: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]\n",
      "numpy: 1.24.4 c:\\Users\\Asus\\Desktop\\DL_Project\\.venv\\lib\\site-packages\\numpy\\__init__.py\n",
      "PIL: 12.0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"python version:\", sys.version)\n",
    "\n",
    "import numpy as np\n",
    "print(\"numpy:\", np.__version__, np.__file__)\n",
    "\n",
    "from PIL import Image\n",
    "print(\"PIL:\", Image.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e7186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded image shape: (41, 41, 3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "img = Image.open(r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\imagesf\\00001.ppm\").convert(\"RGB\")\n",
    "print(\"loaded image shape:\", np.array(img).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc39279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np array shape dtype: (41, 41, 3) uint8\n",
      "tensor shape: torch.Size([3, 41, 41]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np, torch\n",
    "from torchvision import transforms as T\n",
    "img_path = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\imagesf\\00001.ppm\"   # pick a real .ppm image path\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "arr = np.array(img)\n",
    "print(\"np array shape dtype:\", arr.shape, arr.dtype)\n",
    "t = T.ToTensor()(img)\n",
    "print(\"tensor shape:\", t.shape, t.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f718f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset file: c:\\Users\\Asus\\Desktop\\DL_Project\\dataset.py\n",
      "names in dataset module:\n",
      " ['Dataset', 'Image', 'myDataset', 'os', 'torch']\n"
     ]
    }
   ],
   "source": [
    "import importlib, dataset, inspect, sys\n",
    "importlib.reload(dataset)\n",
    "print(\"dataset file:\", dataset.__file__)\n",
    "print(\"names in dataset module:\\n\", sorted([n for n in dir(dataset) if not n.startswith(\"__\")])[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "from dataset import myDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee082209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myDataset: found 1106 images under C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\n",
      "len(ds) = 1106\n",
      "target keys: ['boxes', 'labels', 'image_id', 'area', 'iscrowd']\n",
      "boxes shape: torch.Size([0, 4])\n"
     ]
    }
   ],
   "source": [
    "root = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\"\n",
    "from dataset import myDataset          # ensure module imported after reload\n",
    "from transforms import get_transforms       \n",
    "\n",
    "ds = myDataset(root, transform=get_transforms(train=True))   \n",
    "print(\"len(ds) =\", len(ds))\n",
    "img, target = ds[0]\n",
    "print(\"target keys:\", list(target.keys()))\n",
    "print(\"boxes shape:\", target[\"boxes\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images folder (first 10): ['00000.ppm', '00001.ppm', '00002.ppm', '00003.ppm', '00004.ppm', '00005.ppm', '00006.ppm', '00007.ppm', '00008.ppm', '00009.ppm']\n",
      "imagesf folder (first 10): ['00000.ppm', '00001.ppm', '00002.ppm', '00003.ppm', '00004.ppm', '00005.ppm', '00006.ppm', '00007.ppm', '00008.ppm', '00009.ppm']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"images folder (first 10):\", os.listdir(os.path.join(root, \"images\"))[:10])\n",
    "print(\"imagesf folder (first 10):\", os.listdir(os.path.join(root, \"imagesf\"))[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\n",
      "cwd: c:\\Users\\Asus\\Desktop\\DL_Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"root:\", root)\n",
    "print(\"cwd:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images found under root: 1106\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00000.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00001.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00002.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00003.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00004.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00005.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00006.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00007.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00008.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00009.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00010.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00011.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00012.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00013.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00014.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00015.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00016.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00017.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00018.ppm\n",
      "C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\\images\\00019.ppm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_images(start, exts=(\".jpg\",\".jpeg\",\".png\",\".ppm\",\".bmp\",\".tif\",\".tiff\")):\n",
    "    found = []\n",
    "    for dirpath, dirs, files in os.walk(start):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(exts):\n",
    "                found.append(os.path.join(dirpath, f))\n",
    "    return found\n",
    "\n",
    "# try under the root first\n",
    "imgs = find_images(root)\n",
    "print(\"images found under root:\", len(imgs))\n",
    "print(\"\\n\".join(imgs[:20]))\n",
    "\n",
    "# if none, search from project root (cwd)\n",
    "if len(imgs)==0:\n",
    "    imgs2 = find_images(os.getcwd())\n",
    "    print(\"images found under cwd:\", len(imgs2))\n",
    "    print(\"\\n\".join(imgs2[:50]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c553e3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Setup ---\n",
      "Loading annotations...\n",
      "Loaded 506 annotated images.\n",
      "Creating datasets...\n",
      "myDataset: found 1106 images under C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\n",
      "myDataset: found 1106 images under C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\n",
      "Datasets created.\n",
      "Creating dataloaders...\n",
      "Dataloaders ready.\n",
      "Loading pretrained model...\n",
      "Loaded pretrained model with COCO head\n",
      "Replaced box predictor -> new num_classes = 44\n",
      "Model loaded and moved to device.\n",
      "--- RESUMING TRAINING from model_epoch_9.pth ---\n",
      "Model weights loaded successfully. Ready to train for 10 more epochs.\n",
      "--- SETUP COMPLETE. YOU CAN NOW RUN TRAINING. ---\n"
     ]
    }
   ],
   "source": [
    "# --- THIS IS YOUR FINAL, COMPLETE SETUP CELL ---(FOR TRAINING)\n",
    "# --- (Includes Bulletproof NAN fix, +1 label fix, and correct file imports) ---\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# Import all helper scripts\n",
    "import utils \n",
    "from engine import train_one_epoch, evaluate\n",
    "from dataset import myDataset \n",
    "from transforms import get_transforms \n",
    "import numpy as np\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor \n",
    "import my_collate  # <-- Imports your collate_fn from my_collate.py\n",
    "\n",
    "print(\"--- Starting Setup ---\")\n",
    "\n",
    "# 1. Define device, root, classes\n",
    "os.environ['TORCH_HOME'] = './'\n",
    "root = r\"C:\\Users\\Asus\\Desktop\\DL_Project\\data\\gtsdb\\trains\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# THIS IS THE TOTAL NUMBER OF CLASSES (43) + 1 FOR BACKGROUND\n",
    "num_classes = 44 \n",
    "\n",
    "# 2. Define your transforms\n",
    "transform_train = get_transforms(train=True)\n",
    "transform_test = get_transforms(train=False)\n",
    "\n",
    "# 3. --- YOUR REAL 'dic' CODE (with BULLETPROOF +1 AND NAN FIX) ---\n",
    "print(\"Loading annotations...\")\n",
    "txt = np.genfromtxt(r'C:\\Users\\Asus\\Desktop\\DL Data_Set\\DLDB\\train\\TrainIJCNN2013\\gt.txt',\n",
    "                  delimiter=';', dtype=None, encoding=None) # type: ignore\n",
    "\n",
    "dic = {}\n",
    "bad_boxes_skipped = 0\n",
    "for i in range(len(txt)):\n",
    "    img_name = txt[i][0]\n",
    "    \n",
    "    # --- THIS IS THE +1 LABEL FIX ---\n",
    "    original_label = int(txt[i][5])\n",
    "    new_label = original_label + 1  # Map 0->1, 1->2, ..., 42->43\n",
    "    # --- END OF +1 LABEL FIX ---\n",
    "\n",
    "    # Bounding box coordinates\n",
    "    x1, y1, x2, y2 = int(txt[i][1]), int(txt[i][2]), int(txt[i][3]), int(txt[i][4])\n",
    "    \n",
    "    # --- !!! BULLETPROOF NAN FIX !!! ---\n",
    "    # 1. Force coordinates into (x_min, y_min, x_max, y_max) order\n",
    "    #    This fixes \"inverted boxes\" which cause nan.\n",
    "    x_min = min(x1, x2)\n",
    "    y_min = min(y1, y2)\n",
    "    x_max = max(x1, x2)\n",
    "    y_max = max(y1, y2)\n",
    "\n",
    "    # 2. Check for \"zero-area\" boxes\n",
    "    if x_max <= x_min or y_max <= y_min:\n",
    "        bad_boxes_skipped += 1\n",
    "        continue # Skip this bad annotation\n",
    "    \n",
    "    # This is now a guaranteed-good bounding box\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "    # --- !!! END OF BULLETPROOF NAN FIX !!! ---\n",
    "    \n",
    "    # Store as a DICTIONARY\n",
    "    annotation = {'bbox': bbox, 'label': new_label} \n",
    "    \n",
    "    if img_name in dic:\n",
    "        dic[img_name].append(annotation)\n",
    "    else:\n",
    "        dic[img_name] = [annotation]\n",
    "\n",
    "print(f\"Loaded {len(dic)} annotated images.\")\n",
    "if bad_boxes_skipped > 0:\n",
    "    print(f\"--- WARNING: Skipped {bad_boxes_skipped} bad (zero-area or inverted) boxes. ---\")\n",
    "# --- END OF 'dic' CODE ---\n",
    "\n",
    "\n",
    "# 4. Call myDataset with KEYWORD ARGUMENTS (root=, transform=, dic=)\n",
    "print(\"Creating datasets...\")\n",
    "dataset = myDataset(root=root, transform=transform_train, dic=dic)\n",
    "dataset_test = myDataset(root=root, transform=transform_test, dic=dic)\n",
    "print(\"Datasets created.\")\n",
    "\n",
    "# 5. Split the dataset\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-100])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-100:])\n",
    "\n",
    "# 6. Define training and validation data loaders (Fixed for 4GB GPU & file conflict)\n",
    "print(\"Creating dataloaders...\")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True,  # <-- Kept at 2 to avoid OutOfMemory\n",
    "    num_workers=0, # <-- CRITICAL: Must be 0\n",
    "    collate_fn=my_collate.collate_fn  # <-- FIXED: Using my_collate.py\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=2, shuffle=False, # <-- Kept at 2 to avoid OutOfMemory\n",
    "    num_workers=0, # <-- CRITICAL: Must be 0\n",
    "    collate_fn=my_collate.collate_fn  # <-- FIXED: Using my_collate.py\n",
    ")\n",
    "print(\"Dataloaders ready.\")\n",
    "\n",
    "# 7. --- YOUR NEW, CORRECTED MODEL CODE ---\n",
    "print(\"Loading pretrained model...\")\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "print(\"Loaded pretrained model with COCO head\")\n",
    "\n",
    "# Replace the box predictor with a new one for your num_classes\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "print(f\"Replaced box predictor -> new num_classes = {num_classes}\")\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model loaded and moved to device.\")\n",
    "\n",
    "# --- !!! THIS IS THE FIX TO RESUME TRAINING !!! ---\n",
    "# Load the model you just trained for 10 epochs\n",
    "MODEL_TO_RESUME = 'model_epoch_9.pth' \n",
    "try:\n",
    "    print(f\"--- RESUMING TRAINING from {MODEL_TO_RESUME} ---\")\n",
    "    model.load_state_dict(torch.load(MODEL_TO_RESUME, map_location=device))\n",
    "    print(\"Model weights loaded successfully. Ready to train for 10 more epochs.\")\n",
    "except Exception as e:\n",
    "    print(f\"--- WARNING: Could not load {MODEL_TO_RESUME}. Training from scratch. ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "# --- !!! END OF THE FIX !!! ---\n",
    "\n",
    "print(\"--- SETUP COMPLETE. YOU CAN NOW RUN TRAINING. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a2a02",
   "metadata": {},
   "source": [
    "### Starting the Training\n",
    "Defining all the parameters required for training. (Using SGD as optimizer, Cosine Annealing/Decreasing Warm Restarts as learning rate scheduler which decreases the initial learning rate set in a cosine manner until a restart; the lr is set back to the initial lr and the cycle repeats, number of epochs = 1000)\n",
    "Declaring all the variable to be retrieved from the COCO Evaluation metrics.\n",
    "We start train them model and evaluate the performance on test set.\n",
    "Here, train_one_epoch function in engine.py is used to do the training. The train_one_epoch function returns metric_logger object which we store in 'metrics'. We use the metric_logger's attributes (losses) to append into their respective variables. </br>\n",
    "\n",
    "Then, the evaluate method in CocoEvaluator() in coco_eval.py returns a cocoeval object which is stored in ' '. We use this coco_eval object to retrieve the stats attribute from pycocotools' library's summarize(). We append them to all the stat variables to later plot them after training.\n",
    "\n",
    "There is a lot of gibberish of every iteration. Here's the link to jump to next cell: Next Cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- THIS IS YOUR (Fixed) TRAINING LOOP CELL ---\n",
    "\n",
    "# 1. Setup the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Set a very safe learning rate to guarantee stability\n",
    "optimizer = torch.optim.SGD(params, lr=0.0001, # <-- VERY SAFE LR\n",
    "                            momentum=0.9, \n",
    "                            weight_decay=0.0005)\n",
    "\n",
    "# 2. Setup a learning rate scheduler (optional, but recommended)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# 3. Set the number of epochs to train for\n",
    "num_epochs = 10  # You can change this number\n",
    "\n",
    "print(\"--- STARTING TRAINING ---\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train for one epoch, printing every 10 iterations\n",
    "        # This is the function from engine.py\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        \n",
    "        # Update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # --- !!! THIS IS THE FIX !!! ---\n",
    "        # We are commenting out the broken evaluation call.\n",
    "        # Your model will keep training without crashing.\n",
    "        print(f\"--- Epoch {epoch} complete. Skipping evaluation to avoid 'KeyError: info' bug. ---\")\n",
    "        # evaluate(model, data_loader_test, device=device) \n",
    "        # --- !!! END OF THE FIX !!! ---\n",
    "        \n",
    "        \n",
    "        # --- Optional: Save your model after each epoch ---\n",
    "        # This saves the model so you can use it later\n",
    "        model_save_path = f\"model_epoch_{epoch}.pth\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Saved model to {model_save_path}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training stopped early by user.\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\"*30)\n",
    "print(\"\")\n",
    "print(\"Done!\") # This is the \"Done!\" from your screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158b9500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Helper code (coco_eval.py) loaded ---\n",
      "--- Helper code (coco_utils.py) loaded ---\n",
      "--- STARTING EVALUATION ---\n",
      "Loading trained model from model_epoch_9.pth...\n",
      "Model loaded successfully.\n",
      "Using 'data_loader_test' from your Setup Cell...\n",
      "Converting 100 images from the test set to COCO format...\n",
      "creating index...\n",
      "index created!\n",
      "Conversion to COCO format complete.\n",
      "Successfully converted dataset to COCO format.\n",
      "Running evaluation...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "--- WARNING: Model returned no results for iou_type bbox. Skipping update. ---\n",
      "--- This means your model did not detect any objects in the test set. ---\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "--- WARNING: Model returned no results for iou_type bbox. Skipping update. ---\n",
      "--- This means your model did not detect any objects in the test set. ---\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Accumulating evaluation results for iou_type: bbox...\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.93s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      "--- EVALUATION COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import utils  # This imports your utils.py for SmoothedValue, etc.\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# --- We are pasting all the helper code directly into this cell to avoid ImportErrors ---\n",
    "\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.coco import COCO\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "print(\"--- Helper code (coco_eval.py) loaded ---\")\n",
    "class CocoEvaluator:\n",
    "    # ... (This class code is correct, leaving it as-is) ...\n",
    "    def __init__(self, coco_gt, iou_types):\n",
    "        if not isinstance(iou_types, (list, tuple)):\n",
    "            iou_types = [iou_types]\n",
    "        coco_gt = copy.deepcopy(coco_gt)\n",
    "        self.coco_gt = coco_gt\n",
    "        self.iou_types = iou_types\n",
    "        self.coco_eval = {iou_type: COCOeval(coco_gt, iouType=iou_type) for iou_type in iou_types}\n",
    "        self.img_ids = []\n",
    "        self.eval_imgs = {iou_type: [] for iou_type in iou_types}\n",
    "\n",
    "    def update(self, predictions):\n",
    "        img_ids = list(predictions.keys())\n",
    "        self.img_ids.extend(img_ids)\n",
    "\n",
    "        for iou_type in self.iou_types:\n",
    "            results = self.prepare(predictions, iou_type)\n",
    "            \n",
    "            # --- THIS IS THE FIX (To prevent IndexError) ---\n",
    "            # The 'pycocotools.loadRes' function crashes if 'results' is an empty list.\n",
    "            # We must check for this case first.\n",
    "            if not results:\n",
    "                print(f\"--- WARNING: Model returned no results for iou_type {iou_type}. Skipping update. ---\")\n",
    "                print(\"--- This means your model did not detect any objects in the test set. ---\")\n",
    "                continue  # Skip this iou_type, as there are no results to load\n",
    "            # --- END OF THE FIX ---\n",
    "\n",
    "            # Use loadRes directly (this line is now safe)\n",
    "            self.coco_eval[iou_type].cocoDt = self.coco_gt.loadRes(results)\n",
    "            self.eval_imgs[iou_type] = list(predictions.keys())\n",
    "\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        pass\n",
    "\n",
    "    def accumulate(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            print(f\"Accumulating evaluation results for iou_type: {iou_type}...\")\n",
    "            # Set the image IDs to evaluate\n",
    "            self.coco_eval[iou_type].params.imgIds = self.img_ids\n",
    "            # Run evaluation\n",
    "            self.coco_eval[iou_type].evaluate()\n",
    "            self.coco_eval[iou_type].accumulate()\n",
    "\n",
    "    def summarize(self):\n",
    "        for iou_type in self.iou_types:\n",
    "            print(f\"IoU metric: {iou_type}\")\n",
    "            self.coco_eval[iou_type].summarize()\n",
    "\n",
    "    def prepare(self, predictions, iou_type):\n",
    "        if iou_type == \"bbox\":\n",
    "            return self.prepare_for_coco_detection(predictions)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown iou_type: {iou_type}\")\n",
    "\n",
    "    def prepare_for_coco_detection(self, predictions):\n",
    "        coco_results = []\n",
    "        for original_id, prediction in predictions.items():\n",
    "            if \"boxes\" not in prediction or \"scores\" not in prediction or \"labels\" not in prediction:\n",
    "                continue\n",
    "\n",
    "            boxes = prediction[\"boxes\"].tolist()\n",
    "            scores = prediction[\"scores\"].tolist()\n",
    "            # --- THIS IS THE SYNTAX FIX ---\n",
    "            labels = prediction[\"labels\"].tolist() \n",
    "            # --- END OF THE SYNTAX FIX ---\n",
    "            \n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                box[2] = box[2] - box[0]\n",
    "                box[3] = box[3] - box[1]\n",
    "                \n",
    "                result = {\n",
    "                    \"image_id\": original_id,\n",
    "                    \"category_id\": label,\n",
    "                    \"bbox\": box,\n",
    "                    \"score\": score,\n",
    "                }\n",
    "                coco_results.append(result)\n",
    "        return coco_results\n",
    "\n",
    "print(\"--- Helper code (coco_utils.py) loaded ---\")\n",
    "# This is the new, corrected convert_to_coco_api function\n",
    "def convert_to_coco_api(ds):\n",
    "    coco_ds = COCO()\n",
    "    \n",
    "    # --- THIS IS THE FIX: Added a dummy 'info' key ---\n",
    "    dataset = {\"images\": [], \"categories\": [], \"annotations\": [], \"info\": {}}\n",
    "    # --- END OF THE FIX ---\n",
    "    \n",
    "    categories = set()\n",
    "    \n",
    "    # 'ds' is our Subset of 100 test images.\n",
    "    print(f\"Converting {len(ds)} images from the test set to COCO format...\")\n",
    "    \n",
    "    for img_idx in range(len(ds)):\n",
    "        try:\n",
    "            # Get the image and its targets from the dataset (this calls the Subset's __getitem__)\n",
    "            img, targets = ds[img_idx]\n",
    "            \n",
    "            image_id = targets[\"image_id\"].item()\n",
    "            \n",
    "            img_dict = {}\n",
    "            img_dict[\"id\"] = image_id\n",
    "            img_dict[\"height\"] = img.shape[-2]\n",
    "            img_dict[\"width\"] = img.shape[-1]\n",
    "            dataset[\"images\"].append(img_dict)\n",
    "            \n",
    "            bboxes = targets[\"boxes\"]\n",
    "            if bboxes.shape[0] > 0:\n",
    "                bboxes[:, 2:] -= bboxes[:, :2]\n",
    "                bboxes = bboxes.tolist()\n",
    "                labels = targets[\"labels\"].tolist()\n",
    "                areas = targets[\"area\"].tolist()\n",
    "                iscrowd = targets[\"iscrowd\"].tolist()\n",
    "\n",
    "                for i in range(len(bboxes)):\n",
    "                    ann = {}\n",
    "                    ann[\"image_id\"] = image_id\n",
    "                    ann[\"bbox\"] = bboxes[i]\n",
    "                    ann[\"category_id\"] = labels[i]\n",
    "                    categories.add(labels[i])\n",
    "                    ann[\"area\"] = areas[i]\n",
    "                    ann[\"iscrowd\"] = iscrowd[i]\n",
    "                    ann[\"id\"] = len(dataset[\"annotations\"]) + 1\n",
    "                    dataset[\"annotations\"].append(ann)\n",
    "        \n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    dataset[\"categories\"] = [{\"id\": i, \"name\": str(i), \"supercategory\": \"none\"} for i in sorted(categories)]\n",
    "    \n",
    "    if dataset[\"annotations\"]:\n",
    "        coco_ds.dataset = dataset\n",
    "        coco_ds.createIndex()\n",
    "    else:\n",
    "        print(\"--- WARNING: No annotations found in the test set! ---\")\n",
    "\n",
    "    print(\"Conversion to COCO format complete.\")\n",
    "    return coco_ds\n",
    "\n",
    "# --- THIS IS THE FIXED FUNCTION ---\n",
    "def get_coco_api_from_dataset(dataset):\n",
    "    # 'dataset' is our data_loader.dataset, which is a torch.utils.data.Subset\n",
    "    # We want to convert this Subset directly, NOT unwrap it.\n",
    "    for _ in range(10):\n",
    "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "            break\n",
    "        # We NO LONGER unwrap the Subset\n",
    "        # if isinstance(dataset, torch.utils.data.Subset):\n",
    "        #     dataset = dataset.dataset\n",
    "    \n",
    "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
    "        return dataset.coco\n",
    "        \n",
    "    # 'dataset' is our Subset. We pass it directly.\n",
    "    return convert_to_coco_api(dataset)\n",
    "# --- END OF THE FIX ---\n",
    "\n",
    "\n",
    "# This is our simple evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    n_threads = torch.get_num_threads()\n",
    "    torch.set_num_threads(1)\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # This will now correctly convert our 100-image test set\n",
    "        coco_gt = get_coco_api_from_dataset(data_loader.dataset)\n",
    "        print(\"Successfully converted dataset to COCO format.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting dataset to COCO format: {e}\")\n",
    "        return None\n",
    "\n",
    "    iou_types = [\"bbox\"]\n",
    "    coco_evaluator = CocoEvaluator(coco_gt, iou_types)\n",
    "    \n",
    "    print(\"Running evaluation...\")\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        original_image_ids = [t[\"image_id\"].item() for t in targets]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        outputs = model(images)\n",
    "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
    "        \n",
    "        predictions = {img_id: output for img_id, output in zip(original_image_ids, outputs)}\n",
    "        \n",
    "        coco_evaluator.update(predictions)\n",
    "\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    \n",
    "    torch.set_num_threads(n_threads)\n",
    "    return coco_evaluator\n",
    "\n",
    "# --- Main Evaluation Script ---\n",
    "print(\"--- STARTING EVALUATION ---\")\n",
    "\n",
    "# 1. DEFINE YOUR MODEL PATH\n",
    "# Change this to your best model, e.g., \"model_epoch_9.pth\"\n",
    "MODEL_PATH = \"model_epoch_9.pth\" # <-- !!! EDIT THIS LINE !!!\n",
    "\n",
    "# 2. CREATE A NEW MODEL (to load the weights into)\n",
    "num_classes = 44 \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# 3. LOAD YOUR TRAINED WEIGHTS\n",
    "try:\n",
    "    print(f\"Loading trained model from {MODEL_PATH}...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"--- ERROR LOADING MODEL ---\")\n",
    "    print(f\"Could not load {MODEL_PATH}. Did you train your model and save it?\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 4. RUN THE EVALUATION\n",
    "if \"data_loader_test\" in locals():\n",
    "    print(\"Using 'data_loader_test' from your Setup Cell...\")\n",
    "    evaluate_model(model, data_loader_test, device=device)\n",
    "else:\n",
    "    print(\"--- ERROR ---\")\n",
    "    print(\"Variable 'data_loader_test' not found.\")\n",
    "    print(\"Please run your Setup Cell first to create the test data loader.\")\n",
    "\n",
    "print(\"--- EVALUATION COMPLETE ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
